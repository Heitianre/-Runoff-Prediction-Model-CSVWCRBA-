{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "655a1eeb",
   "metadata": {},
   "source": [
    "### CEEMDAN-SSA-VMD--WOA-CNN-ResNet-BiGRU-Attention（The hyperparameters have been optimized.）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b205a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from PyEMD import CEEMDAN  # 确保你已经安装了PyEMD库\n",
    "from vmdpy import VMD  # 确保你已经安装了vmdpy库\n",
    "\n",
    "# 固定随机种子\n",
    "seed = 42  # 可以选择任意整数作为种子\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# 设置设备\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 读取数据\n",
    "data = pd.read_csv('D:/Jupyter/Data/Source code data/单一特征/龙门径流.csv')['flow'].values\n",
    "# CCEMDAN分解\n",
    "ceemdan = CEEMDAN()\n",
    "IMFs = ceemdan(data)\n",
    "K_ceemdan = len(IMFs)  # CCEMDAN模态数\n",
    "\n",
    "# 对第一个和第二个模态进行VMD分解\n",
    "alpha = 1565  # 带宽限制\n",
    "tau = 0  # 时间平滑参数\n",
    "K_vmd = 7  # VMD分解的模态数\n",
    "DC = 0  # 直流成分\n",
    "init = 1  # 初始化方法\n",
    "tol = 1e-7  # 收敛容差\n",
    "\n",
    "u1, u_hat1, omega1 = VMD(IMFs[0], alpha, tau, K_vmd, DC, init, tol)\n",
    "u2, u_hat2, omega2 = VMD(IMFs[1], alpha, tau, K_vmd, DC, init, tol)\n",
    "\n",
    "# 数据预处理\n",
    "scalers_ceemdan = [MinMaxScaler() for _ in range(K_ceemdan)]\n",
    "data_scaled_ceemdan = [scalers_ceemdan[i].fit_transform(IMFs[i].reshape(-1, 1)) for i in range(K_ceemdan)]\n",
    "\n",
    "scalers_vmd1 = [MinMaxScaler() for _ in range(K_vmd)]\n",
    "data_scaled_vmd1 = [scalers_vmd1[i].fit_transform(u1[i].reshape(-1, 1)) for i in range(K_vmd)]\n",
    "\n",
    "scalers_vmd2 = [MinMaxScaler() for _ in range(K_vmd)]\n",
    "data_scaled_vmd2 = [scalers_vmd2[i].fit_transform(u2[i].reshape(-1, 1)) for i in range(K_vmd)]\n",
    "\n",
    "# 创建时间窗口数据集\n",
    "def create_dataset(dataset, look_back=12):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset) - look_back):\n",
    "        a = dataset[i:(i + look_back), 0]\n",
    "        dataX.append(a)\n",
    "        dataY.append(dataset[i + look_back, 0])\n",
    "    return np.array(dataX), np.array(dataY)\n",
    "\n",
    "look_back = 12\n",
    "Xs_ceemdan, ys_ceemdan = [], []\n",
    "for i in range(K_ceemdan):\n",
    "    X, y = create_dataset(data_scaled_ceemdan[i], look_back)\n",
    "    Xs_ceemdan.append(X)\n",
    "    ys_ceemdan.append(y)\n",
    "\n",
    "Xs_vmd1, ys_vmd1 = [], []\n",
    "for i in range(K_vmd):\n",
    "    X, y = create_dataset(data_scaled_vmd1[i], look_back)\n",
    "    Xs_vmd1.append(X)\n",
    "    ys_vmd1.append(y)\n",
    "\n",
    "Xs_vmd2, ys_vmd2 = [], []\n",
    "for i in range(K_vmd):\n",
    "    X, y = create_dataset(data_scaled_vmd2[i], look_back)\n",
    "    Xs_vmd2.append(X)\n",
    "    ys_vmd2.append(y)\n",
    "\n",
    "# 划分数据集\n",
    "# 划分数据集\n",
    "X_trains_ceemdan, X_vals_ceemdan, X_tests_ceemdan, y_trains_ceemdan, y_vals_ceemdan, y_tests_ceemdan = [], [], [], [], [], []\n",
    "for i in range(K_ceemdan):\n",
    "    data_length = len(Xs_ceemdan[i])\n",
    "    train_end = int(data_length * 0.7)\n",
    "    val_end = int(data_length * 0.85)\n",
    "    X_train = Xs_ceemdan[i][:train_end]\n",
    "    X_val = Xs_ceemdan[i][train_end:val_end]\n",
    "    X_test = Xs_ceemdan[i][val_end:]\n",
    "    y_train = ys_ceemdan[i][:train_end]\n",
    "    y_val = ys_ceemdan[i][train_end:val_end]\n",
    "    y_test = ys_ceemdan[i][val_end:]\n",
    "    X_trains_ceemdan.append(X_train)\n",
    "    X_vals_ceemdan.append(X_val)\n",
    "    X_tests_ceemdan.append(X_test)\n",
    "    y_trains_ceemdan.append(y_train)\n",
    "    y_vals_ceemdan.append(y_val)\n",
    "    y_tests_ceemdan.append(y_test)\n",
    "\n",
    "X_trains_vmd1, X_vals_vmd1, X_tests_vmd1, y_trains_vmd1, y_vals_vmd1, y_tests_vmd1 = [], [], [], [], [], []\n",
    "for i in range(K_vmd):\n",
    "    data_length = len(Xs_vmd1[i])\n",
    "    train_end = int(data_length * 0.7)\n",
    "    val_end = int(data_length * 0.85)\n",
    "    X_train = Xs_vmd1[i][:train_end]\n",
    "    X_val = Xs_vmd1[i][train_end:val_end]\n",
    "    X_test = Xs_vmd1[i][val_end:]\n",
    "    y_train = ys_vmd1[i][:train_end]\n",
    "    y_val = ys_vmd1[i][train_end:val_end]\n",
    "    y_test = ys_vmd1[i][val_end:]\n",
    "    X_trains_vmd1.append(X_train)\n",
    "    X_vals_vmd1.append(X_val)\n",
    "    X_tests_vmd1.append(X_test)\n",
    "    y_trains_vmd1.append(y_train)\n",
    "    y_vals_vmd1.append(y_val)\n",
    "    y_tests_vmd1.append(y_test)\n",
    "\n",
    "X_trains_vmd2, X_vals_vmd2, X_tests_vmd2, y_trains_vmd2, y_vals_vmd2, y_tests_vmd2 = [], [], [], [], [], []\n",
    "for i in range(K_vmd):\n",
    "    data_length = len(Xs_vmd2[i])\n",
    "    train_end = int(data_length * 0.7)\n",
    "    val_end = int(data_length * 0.85)\n",
    "    X_train = Xs_vmd2[i][:train_end]\n",
    "    X_val = Xs_vmd2[i][train_end:val_end]\n",
    "    X_test = Xs_vmd2[i][val_end:]\n",
    "    y_train = ys_vmd2[i][:train_end]\n",
    "    y_val = ys_vmd2[i][train_end:val_end]\n",
    "    y_test = ys_vmd2[i][val_end:]\n",
    "    X_trains_vmd2.append(X_train)\n",
    "    X_vals_vmd2.append(X_val)\n",
    "    X_tests_vmd2.append(X_test)\n",
    "    y_trains_vmd2.append(y_train)\n",
    "    y_vals_vmd2.append(y_val)\n",
    "    y_tests_vmd2.append(y_test)\n",
    "# 转换为Tensor\n",
    "X_trains_ceemdan = [torch.tensor(X_train, dtype=torch.float32).unsqueeze(2).to(device) for X_train in X_trains_ceemdan]\n",
    "y_trains_ceemdan = [torch.tensor(y_train, dtype=torch.float32).unsqueeze(1).to(device) for y_train in y_trains_ceemdan]\n",
    "X_vals_ceemdan = [torch.tensor(X_val, dtype=torch.float32).unsqueeze(2).to(device) for X_val in X_vals_ceemdan]\n",
    "y_vals_ceemdan = [torch.tensor(y_val, dtype=torch.float32).unsqueeze(1).to(device) for y_val in y_vals_ceemdan]\n",
    "X_tests_ceemdan = [torch.tensor(X_test, dtype=torch.float32).unsqueeze(2).to(device) for X_test in X_tests_ceemdan]\n",
    "y_tests_ceemdan = [torch.tensor(y_test, dtype=torch.float32).unsqueeze(1).to(device) for y_test in y_tests_ceemdan]\n",
    "\n",
    "X_trains_vmd1 = [torch.tensor(X_train, dtype=torch.float32).unsqueeze(2).to(device) for X_train in X_trains_vmd1]\n",
    "y_trains_vmd1 = [torch.tensor(y_train, dtype=torch.float32).unsqueeze(1).to(device) for y_train in y_trains_vmd1]\n",
    "X_vals_vmd1 = [torch.tensor(X_val, dtype=torch.float32).unsqueeze(2).to(device) for X_val in X_vals_vmd1]\n",
    "y_vals_vmd1 = [torch.tensor(y_val, dtype=torch.float32).unsqueeze(1).to(device) for y_val in y_vals_vmd1]\n",
    "X_tests_vmd1 = [torch.tensor(X_test, dtype=torch.float32).unsqueeze(2).to(device) for X_test in X_tests_vmd1]\n",
    "y_tests_vmd1 = [torch.tensor(y_test, dtype=torch.float32).unsqueeze(1).to(device) for y_test in y_tests_vmd1]\n",
    "\n",
    "X_trains_vmd2 = [torch.tensor(X_train, dtype=torch.float32).unsqueeze(2).to(device) for X_train in X_trains_vmd2]\n",
    "y_trains_vmd2 = [torch.tensor(y_train, dtype=torch.float32).unsqueeze(1).to(device) for y_train in y_trains_vmd2]\n",
    "X_vals_vmd2 = [torch.tensor(X_val, dtype=torch.float32).unsqueeze(2).to(device) for X_val in X_vals_vmd2]\n",
    "y_vals_vmd2 = [torch.tensor(y_val, dtype=torch.float32).unsqueeze(1).to(device) for y_val in y_vals_vmd2]\n",
    "X_tests_vmd2 = [torch.tensor(X_test, dtype=torch.float32).unsqueeze(2).to(device) for X_test in X_tests_vmd2]\n",
    "y_tests_vmd2 = [torch.tensor(y_test, dtype=torch.float32).unsqueeze(1).to(device) for y_test in y_tests_vmd2]\n",
    "\n",
    "# 创建DataLoader\n",
    "train_loaders_ceemdan = [DataLoader(TensorDataset(X_trains_ceemdan[i], y_trains_ceemdan[i]), batch_size=32, shuffle=True) for i in range(K_ceemdan)]\n",
    "val_loaders_ceemdan = [DataLoader(TensorDataset(X_vals_ceemdan[i], y_vals_ceemdan[i]), batch_size=32, shuffle=False) for i in range(K_ceemdan)]\n",
    "test_loaders_ceemdan = [DataLoader(TensorDataset(X_tests_ceemdan[i], y_tests_ceemdan[i]), batch_size=32, shuffle=False) for i in range(K_ceemdan)]\n",
    "\n",
    "train_loaders_vmd1 = [DataLoader(TensorDataset(X_trains_vmd1[i], y_trains_vmd1[i]), batch_size=32, shuffle=True) for i in range(K_vmd)]\n",
    "val_loaders_vmd1 = [DataLoader(TensorDataset(X_vals_vmd1[i], y_vals_vmd1[i]), batch_size=32, shuffle=False) for i in range(K_vmd)]\n",
    "test_loaders_vmd1 = [DataLoader(TensorDataset(X_tests_vmd1[i], y_tests_vmd1[i]), batch_size=32, shuffle=False) for i in range(K_vmd)]\n",
    "\n",
    "train_loaders_vmd2 = [DataLoader(TensorDataset(X_trains_vmd2[i], y_trains_vmd2[i]), batch_size=32, shuffle=True) for i in range(K_vmd)]\n",
    "val_loaders_vmd2 = [DataLoader(TensorDataset(X_vals_vmd2[i], y_vals_vmd2[i]), batch_size=32, shuffle=False) for i in range(K_vmd)]\n",
    "test_loaders_vmd2 = [DataLoader(TensorDataset(X_tests_vmd2[i], y_tests_vmd2[i]), batch_size=32, shuffle=False) for i in range(K_vmd)]\n",
    "\n",
    "# 定义多头注意力模块\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.Wq = nn.Linear(input_dim, hidden_dim * num_heads)\n",
    "        self.Wk = nn.Linear(input_dim, hidden_dim * num_heads)\n",
    "        self.Wv = nn.Linear(input_dim, hidden_dim * num_heads)\n",
    "        self.fc = nn.Linear(hidden_dim * num_heads, input_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_length, _ = x.size()\n",
    "        \n",
    "        # 生成 Q, K, V\n",
    "        Q = self.Wq(x).view(batch_size, seq_length, self.num_heads, self.hidden_dim).transpose(1, 2)  # (batch_size, num_heads, seq_length, hidden_dim)\n",
    "        K = self.Wk(x).view(batch_size, seq_length, self.num_heads, self.hidden_dim).transpose(1, 2)\n",
    "        V = self.Wv(x).view(batch_size, seq_length, self.num_heads, self.hidden_dim).transpose(1, 2)\n",
    "        \n",
    "        # 计算注意力分数\n",
    "        attention_scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.hidden_dim ** 0.5)  # (batch_size, num_heads, seq_length, seq_length)\n",
    "        attention_weights = torch.softmax(attention_scores, dim=-1)  # (batch_size, num_heads, seq_length, seq_length)\n",
    "        \n",
    "        # 加权平均\n",
    "        context = torch.matmul(attention_weights, V)  # (batch_size, num_heads, seq_length, hidden_dim)\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, seq_length, -1)  # (batch_size, seq_length, hidden_dim * num_heads)\n",
    "        \n",
    "        output = self.fc(context)  # (batch_size, seq_length, input_dim)\n",
    "        return output\n",
    "\n",
    "# 定义CNN-Resnet-BiGRU-MultiHeadAttention模型\n",
    "class CNN_Resnet_BiGRU_MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim, num_heads):\n",
    "        super(CNN_Resnet_BiGRU_MultiHeadAttention, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # CNN部分\n",
    "        self.conv1 = nn.Conv1d(input_dim, 64, kernel_size=3, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2)\n",
    "        self.conv2 = nn.Conv1d(64, 128, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv1d(128, 256, kernel_size=3, padding=1)\n",
    "        \n",
    "        # BiGRU部分\n",
    "        self.bigru = nn.GRU(256, hidden_dim, num_layers, batch_first=True, bidirectional=True)\n",
    "        \n",
    "        # 多头注意力部分\n",
    "        self.multihead_attention = MultiHeadAttention(hidden_dim * 2, hidden_dim, num_heads)\n",
    "        \n",
    "        # 全连接层\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 调整张量的维度顺序\n",
    "        x = x.permute(0, 2, 1)  # 从 [batch_size, sequence_length, input_dim] 变为 [batch_size, input_dim, sequence_length]\n",
    "        \n",
    "        # CNN部分\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        # 调整形状以适应BiGRU输入\n",
    "        x = x.permute(0, 2, 1)\n",
    "        \n",
    "        # BiGRU部分\n",
    "        h0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_dim).to(device)\n",
    "        out, _ = self.bigru(x, h0)\n",
    "        \n",
    "        # 多头注意力部分\n",
    "        out = self.multihead_attention(out)\n",
    "        \n",
    "        # 选择最后一个时间步的输出\n",
    "        out = out[:, -1, :]\n",
    "        \n",
    "        # 全连接层\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "input_dim = 1\n",
    "hidden_dim = 64\n",
    "num_layers = 2\n",
    "output_dim = 1\n",
    "num_heads = 4  # 设置多头注意力的头数\n",
    "models_ceemdan = [CNN_Resnet_BiGRU_MultiHeadAttention(input_dim, hidden_dim, num_layers, output_dim, num_heads).to(device) for _ in range(K_ceemdan)]\n",
    "models_vmd1 = [CNN_Resnet_BiGRU_MultiHeadAttention(input_dim, hidden_dim, num_layers, output_dim, num_heads).to(device) for _ in range(K_vmd)]\n",
    "models_vmd2 = [CNN_Resnet_BiGRU_MultiHeadAttention(input_dim, hidden_dim, num_layers, output_dim, num_heads).to(device) for _ in range(K_vmd)]\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.MSELoss()\n",
    "optimizers_ceemdan = [optim.Adam(models_ceemdan[i].parameters(), lr=0.0006) for i in range(K_ceemdan)]\n",
    "optimizers_vmd1 = [optim.Adam(models_vmd1[i].parameters(), lr=0.0006) for i in range(K_vmd)]\n",
    "optimizers_vmd2 = [optim.Adam(models_vmd2[i].parameters(), lr=0.0006) for i in range(K_vmd)]\n",
    "\n",
    "# 训练模型\n",
    "num_epochs = 150\n",
    "train_losses_ceemdan = [[] for _ in range(K_ceemdan)]\n",
    "val_losses_ceemdan = [[] for _ in range(K_ceemdan)]\n",
    "train_losses_vmd1 = [[] for _ in range(K_vmd)]\n",
    "val_losses_vmd1 = [[] for _ in range(K_vmd)]\n",
    "train_losses_vmd2 = [[] for _ in range(K_vmd)]\n",
    "val_losses_vmd2 = [[] for _ in range(K_vmd)]\n",
    "\n",
    "for i in range(K_ceemdan):\n",
    "    for epoch in range(num_epochs):\n",
    "        models_ceemdan[i].train()\n",
    "        epoch_train_loss = 0.0\n",
    "        for j, (inputs, labels) in enumerate(train_loaders_ceemdan[i]):\n",
    "            optimizers_ceemdan[i].zero_grad()\n",
    "            outputs = models_ceemdan[i](inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizers_ceemdan[i].step()\n",
    "            epoch_train_loss += loss.item()\n",
    "        train_losses_ceemdan[i].append(epoch_train_loss / len(train_loaders_ceemdan[i]))\n",
    "        models_ceemdan[i].eval()\n",
    "        epoch_val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loaders_ceemdan[i]:\n",
    "                outputs = models_ceemdan[i](inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                epoch_val_loss += loss.item()\n",
    "            val_losses_ceemdan[i].append(epoch_val_loss / len(val_loaders_ceemdan[i]))\n",
    "\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'CCEMDAN IMF {i+1} - Epoch [{epoch + 1}/{num_epochs}], Train Loss: {train_losses_ceemdan[i][-1]:.4f}, Val Loss: {val_losses_ceemdan[i][-1]:.4f}')\n",
    "\n",
    "for i in range(K_vmd):\n",
    "    for epoch in range(num_epochs):\n",
    "        models_vmd1[i].train()\n",
    "        epoch_train_loss = 0.0\n",
    "        for j, (inputs, labels) in enumerate(train_loaders_vmd1[i]):\n",
    "            optimizers_vmd1[i].zero_grad()\n",
    "            outputs = models_vmd1[i](inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizers_vmd1[i].step()\n",
    "            epoch_train_loss += loss.item()\n",
    "        train_losses_vmd1[i].append(epoch_train_loss / len(train_loaders_vmd1[i]))\n",
    "\n",
    "        models_vmd1[i].eval()\n",
    "        epoch_val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loaders_vmd1[i]:\n",
    "                outputs = models_vmd1[i](inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                epoch_val_loss += loss.item()\n",
    "            val_losses_vmd1[i].append(epoch_val_loss / len(val_loaders_vmd1[i]))\n",
    "\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'VMD1 IMF {i+1} - Epoch [{epoch + 1}/{num_epochs}], Train Loss: {train_losses_vmd1[i][-1]:.4f}, Val Loss: {val_losses_vmd1[i][-1]:.4f}')\n",
    "\n",
    "for i in range(K_vmd):\n",
    "    for epoch in range(num_epochs):\n",
    "        models_vmd2[i].train()\n",
    "        epoch_train_loss = 0.0\n",
    "        for j, (inputs, labels) in enumerate(train_loaders_vmd2[i]):\n",
    "            optimizers_vmd2[i].zero_grad()\n",
    "            outputs = models_vmd2[i](inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizers_vmd2[i].step()\n",
    "            epoch_train_loss += loss.item()\n",
    "        train_losses_vmd2[i].append(epoch_train_loss / len(train_loaders_vmd2[i]))\n",
    "\n",
    "        models_vmd2[i].eval()\n",
    "        epoch_val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loaders_vmd2[i]:\n",
    "                outputs = models_vmd2[i](inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                epoch_val_loss += loss.item()\n",
    "            val_losses_vmd2[i].append(epoch_val_loss / len(val_loaders_vmd2[i]))\n",
    "\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'VMD2 IMF {i+1} - Epoch [{epoch + 1}/{num_epochs}], Train Loss: {train_losses_vmd2[i][-1]:.4f}, Val Loss: {val_losses_vmd2[i][-1]:.4f}')\n",
    "\n",
    "# 测试模型\n",
    "y_preds_ceemdan = []\n",
    "for i in range(K_ceemdan):\n",
    "    models_ceemdan[i].eval()\n",
    "    y_pred = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, _ in test_loaders_ceemdan[i]:\n",
    "            outputs = models_ceemdan[i](inputs)\n",
    "            y_pred.extend(outputs.cpu().numpy())\n",
    "    y_pred = np.array(y_pred)\n",
    "    y_preds_ceemdan.append(y_pred)\n",
    "\n",
    "y_preds_vmd1 = []\n",
    "for i in range(K_vmd):\n",
    "    models_vmd1[i].eval()\n",
    "    y_pred = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, _ in test_loaders_vmd1[i]:\n",
    "            outputs = models_vmd1[i](inputs)\n",
    "            y_pred.extend(outputs.cpu().numpy())\n",
    "    y_pred = np.array(y_pred)\n",
    "    y_preds_vmd1.append(y_pred)\n",
    "\n",
    "y_preds_vmd2 = []\n",
    "for i in range(K_vmd):\n",
    "    models_vmd2[i].eval()\n",
    "    y_pred = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, _ in test_loaders_vmd2[i]:\n",
    "            outputs = models_vmd2[i](inputs)\n",
    "            y_pred.extend(outputs.cpu().numpy())\n",
    "    y_pred = np.array(y_pred)\n",
    "    y_preds_vmd2.append(y_pred)\n",
    "\n",
    "# 反归一化\n",
    "y_preds_denormalized_ceemdan = [scalers_ceemdan[i].inverse_transform(y_preds_ceemdan[i]) for i in range(K_ceemdan)]\n",
    "y_tests_denormalized_ceemdan = [scalers_ceemdan[i].inverse_transform(y_tests_ceemdan[i].cpu().numpy()) for i in range(K_ceemdan)]\n",
    "\n",
    "y_preds_denormalized_vmd1 = [scalers_vmd1[i].inverse_transform(y_preds_vmd1[i]) for i in range(K_vmd)]\n",
    "y_tests_denormalized_vmd1 = [scalers_vmd1[i].inverse_transform(y_tests_vmd1[i].cpu().numpy()) for i in range(K_vmd)]\n",
    "\n",
    "y_preds_denormalized_vmd2 = [scalers_vmd2[i].inverse_transform(y_preds_vmd2[i]) for i in range(K_vmd)]\n",
    "y_tests_denormalized_vmd2 = [scalers_vmd2[i].inverse_transform(y_tests_vmd2[i].cpu().numpy()) for i in range(K_vmd)]\n",
    "\n",
    "# 重构预测结果\n",
    "final_prediction_ceemdan = np.sum(y_preds_denormalized_ceemdan[2:], axis=0)  # 排除CCEMDAN的前两个模态\n",
    "final_true_ceemdan = np.sum(y_tests_denormalized_ceemdan[2:], axis=0)  # 排除CCEMDAN的前两个模态\n",
    "\n",
    "final_prediction_vmd1 = np.sum(y_preds_denormalized_vmd1, axis=0)\n",
    "final_true_vmd1 = np.sum(y_tests_denormalized_vmd1, axis=0)\n",
    "\n",
    "final_prediction_vmd2 = np.sum(y_preds_denormalized_vmd2, axis=0)\n",
    "final_true_vmd2 = np.sum(y_tests_denormalized_vmd2, axis=0)\n",
    "\n",
    "# 合并CCEMDAN和VMD的预测结果\n",
    "final_prediction = final_prediction_ceemdan + final_prediction_vmd1 + final_prediction_vmd2\n",
    "final_true = final_true_ceemdan + final_true_vmd1 + final_true_vmd2\n",
    "\n",
    "# 保存预测结果\n",
    "predicted_column = 'CCEMDAN-SSA-VMD-WOA-CNN-Resnet-BiGRU-MultiHeadAttention'\n",
    "os.makedirs('D:/Jupyter/A小论文/Result', exist_ok=True)\n",
    "predictions_path = os.path.join('D:/Jupyter/A小论文/Result/预测值', 'predictions.csv')\n",
    "\n",
    "if os.path.exists(predictions_path):\n",
    "    df = pd.read_csv(predictions_path)\n",
    "else:\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "df.loc[:, predicted_column] = final_prediction.flatten()\n",
    "df.to_csv(predictions_path, index=False)\n",
    "\n",
    "# 计算评价指标\n",
    "r2 = r2_score(final_true, final_prediction)\n",
    "mape = mean_absolute_percentage_error(final_true, final_prediction)\n",
    "mse = mean_squared_error(final_true, final_prediction)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print(f'R^2: {r2:.4f}')\n",
    "print(f'MAPE: {mape:.4f}')\n",
    "print(f'MSE: {mse:.4f}')\n",
    "print(f'RMSE: {rmse:.4f}')\n",
    "\n",
    "# 保存模型评价指标\n",
    "evaluation_path = os.path.join('D:/Jupyter/A小论文/Result/模型评价指标', 'model_evaluation.csv')\n",
    "\n",
    "if os.path.exists(evaluation_path):\n",
    "    df_eval = pd.read_csv(evaluation_path)\n",
    "else:\n",
    "    df_eval = pd.DataFrame(columns=['Model', 'R^2', 'MAPE', 'MSE', 'RMSE'])\n",
    "\n",
    "if 'CCEMDAN-SSA-VMD-WOA-CNN-Resnet-BiGRU-MultiHeadAttention' in df_eval['Model'].values:\n",
    "    df_eval.loc[df_eval['Model'] == 'CCEMDAN-SSA-VMD-WOA-CNN-Resnet-BiGRU-MultiHeadAttention', ['R^2', 'MAPE', 'MSE', 'RMSE']] = [r2, mape, mse, rmse]\n",
    "else:\n",
    "    new_row = pd.DataFrame({'Model': ['CCEMDAN-SSA-VMD-WOA-CNN-Resnet-BiGRU-MultiHeadAttention'], 'R^2': [r2], 'MAPE': [mape], 'MSE': [mse], 'RMSE': [rmse]})\n",
    "    df_eval = pd.concat([df_eval, new_row], ignore_index=True)\n",
    "\n",
    "df_eval.to_csv(evaluation_path, index=False)\n",
    "\n",
    "# 预测未来12个月的数据\n",
    "def predict_next_12_months(model, input_data):\n",
    "    input_sequence = input_data.clone()\n",
    "    \n",
    "    # 预测未来12个月的数据\n",
    "    future_predictions = []\n",
    "    for _ in range(12):\n",
    "        predictions = model(input_sequence[-1].unsqueeze(0))  # 假设 model 是一个 PyTorch 模型\n",
    "        next_data = torch.cat((input_sequence[-1, 1:], predictions[0].unsqueeze(0)), dim=0)\n",
    "        input_sequence = torch.cat((input_sequence, next_data.unsqueeze(0)), dim=0)\n",
    "        future_predictions.append(predictions.squeeze().cpu().detach().numpy())\n",
    "    future_predictions = np.array(future_predictions)\n",
    "\n",
    "    return future_predictions\n",
    "\n",
    "# 假设 X_tests 是一个 PyTorch 张量\n",
    "future_predictions_ceemdan = [predict_next_12_months(models_ceemdan[i], X_tests_ceemdan[i][-1:]) for i in range(2, K_ceemdan)]  # 排除CCEMDAN的前两个模态\n",
    "future_predictions_vmd1 = [predict_next_12_months(models_vmd1[i], X_tests_vmd1[i][-1:]) for i in range(K_vmd)]\n",
    "future_predictions_vmd2 = [predict_next_12_months(models_vmd2[i], X_tests_vmd2[i][-1:]) for i in range(K_vmd)]\n",
    "\n",
    "# 反归一化预测结果\n",
    "future_predictions_denormalized_ceemdan = [scalers_ceemdan[i+2].inverse_transform(future_predictions_ceemdan[i].reshape(-1, 1)) for i in range(K_ceemdan-2)]  # 排除CCEMDAN的前两个模态\n",
    "future_predictions_denormalized_vmd1 = [scalers_vmd1[i].inverse_transform(future_predictions_vmd1[i].reshape(-1, 1)) for i in range(K_vmd)]\n",
    "future_predictions_denormalized_vmd2 = [scalers_vmd2[i].inverse_transform(future_predictions_vmd2[i].reshape(-1, 1)) for i in range(K_vmd)]\n",
    "\n",
    "# 重构预测结果\n",
    "final_future_prediction_ceemdan = np.sum(future_predictions_denormalized_ceemdan, axis=0)\n",
    "final_future_prediction_vmd1 = np.sum(future_predictions_denormalized_vmd1, axis=0)\n",
    "final_future_prediction_vmd2 = np.sum(future_predictions_denormalized_vmd2, axis=0)\n",
    "\n",
    "# 合并CCEMDAN和VMD的未来预测结果\n",
    "final_future_prediction = final_future_prediction_ceemdan + final_future_prediction_vmd1 + final_future_prediction_vmd2\n",
    "\n",
    "# 保存预测结果\n",
    "predicted_column = 'CCEMDAN-SSA-VMD-WOA-CNN-Resnet-BiGRU-MultiHeadAttention'\n",
    "os.makedirs('D:/Jupyter/A小论文/Result', exist_ok=True)\n",
    "predictions_path = os.path.join('D:/Jupyter/A小论文/Result/预测值', 'future_predictions.csv')\n",
    "\n",
    "if os.path.exists(predictions_path):\n",
    "    df = pd.read_csv(predictions_path)\n",
    "else:\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "df.loc[:, predicted_column] = final_future_prediction.flatten()\n",
    "df.to_csv(predictions_path, index=False)\n",
    "\n",
    "# 可视化结果\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# 绘制测试集的真实数据和预测数据\n",
    "plt.plot(final_true, label='True (Test Set)', marker='o')\n",
    "plt.plot(final_prediction, label='Predicted (Test Set)', marker='x')\n",
    "\n",
    "# 绘制预测的未来12个月\n",
    "plt.plot(range(len(final_true), len(final_true) + 12), final_future_prediction, label='Predicted Future 12 Months', marker='x')\n",
    "\n",
    "plt.title('True vs Predicted Data')\n",
    "plt.xlabel('Time Steps')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# 可视化损失函数随着训练次数的变化\n",
    "plt.figure(figsize=(12, 6))\n",
    "for i in range(2, K_ceemdan):  # 排除CCEMDAN的前两个模态\n",
    "    plt.plot(train_losses_ceemdan[i], label=f'Train Loss CCEMDAN IMF {i+1}')\n",
    "    plt.plot(val_losses_ceemdan[i], label=f'Validation Loss CCEMDAN IMF {i+1}')\n",
    "for i in range(K_vmd):\n",
    "    plt.plot(train_losses_vmd1[i], label=f'Train Loss VMD1 IMF {i+1}')\n",
    "    plt.plot(val_losses_vmd1[i], label=f'Validation Loss VMD1 IMF {i+1}')\n",
    "for i in range(K_vmd):\n",
    "    plt.plot(train_losses_vmd2[i], label=f'Train Loss VMD2 IMF {i+1}')\n",
    "    plt.plot(val_losses_vmd2[i], label=f'Validation Loss VMD2 IMF {i+1}')\n",
    "plt.title('Loss over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
